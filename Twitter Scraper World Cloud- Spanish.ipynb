{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import tweepy\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Sign up on https://developer.twitter.com/ to create your app and get your credentials\r\n",
    "\r\n",
    "consumer_key =\" \"\r\n",
    "consumer_secret =\" \"\r\n",
    "access_token =\" \" \r\n",
    "access_token_secret =\" \" "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\r\n",
    "auth.set_access_token(access_token, access_token_secret)\r\n",
    "api = tweepy.API(auth)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "number_of_tweets = 300\r\n",
    "tweets = []\r\n",
    "likes = []\r\n",
    "time = []\r\n",
    "\r\n",
    "for i in tweepy.Cursor(api.search, q=\" \", geocode=\"latitude,longitude,radius\", tweet_mode=\"extended\").items(number_of_tweets):\r\n",
    "    tweets.append(i.full_text)\r\n",
    "    likes.append(i.favorite_count)\r\n",
    "    time.append(i.created_at)\r\n",
    "\r\n",
    "#In q=, write inside the \" \" your specific word or phrase that you want to search for\r\n",
    "#In geocode=, replace \"latitude, longitude, radius\" with the area coordinates that you want to search for, or delete it if you do not need it.\r\n",
    "#Radius units must be specified as km (for kilometers) or mi (for miles) \r\n",
    "#There is a helpful website to get coordinates https://www.mapdevelopers.com/draw-circle-tool.php "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Creating a dataframe \r\n",
    "df = pd.DataFrame({'tweets':tweets,'likes':likes,'time':time})"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Removing RTs\r\n",
    "df = df[~df.tweets.str.contains(\"RT\")]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df = df.reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Normalization\r\n",
    "df['tweets'] = df['tweets'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\r\n",
    "\r\n",
    "#Changing tweets to lowercase\r\n",
    "df['tweets'] = df['tweets'].str.lower()\r\n",
    "\r\n",
    "#Removing words with less than 3 characters \r\n",
    "df['tweets'] = df['tweets'].str.replace(r'\\b(\\w{1,3})\\b', '',regex=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "from wordcloud import WordCloud\r\n",
    "import re\r\n",
    "import spacy\r\n",
    "nlp= spacy.load('es_core_news_sm')\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "#If you need the english pipeline replace \"es_core_news_sm\" with \"en_core_web_sm\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Splitting all the sentences\r\n",
    "list_of_sentences = [sentence for sentence in df.tweets]\r\n",
    "\r\n",
    "lines = []\r\n",
    "\r\n",
    "for sentence in list_of_sentences:\r\n",
    "    words = sentence.split()\r\n",
    "    for w in words:\r\n",
    "        lines.append(w)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Removing punctuation\r\n",
    "lines = [re.sub(r'[^A-Za-z0-9]+', '', x) for x in lines]\r\n",
    "\r\n",
    "lines2 = []\r\n",
    "\r\n",
    "for word in lines:\r\n",
    "    if word !='':\r\n",
    "        lines2.append(word)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Removing stop words\r\n",
    "stem2 = []\r\n",
    "\r\n",
    "for word in lines2:\r\n",
    "    if word not in nlp.Defaults.stop_words:\r\n",
    "        stem2.append(word)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df2 = pd.DataFrame(stem2)\r\n",
    "\r\n",
    "df2 = df2[0].value_counts()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "wordcloud = WordCloud(width = 800, height = 800, \r\n",
    "                background_color ='black', \r\n",
    "                max_words=1000,\r\n",
    "                relative_scaling=0.21,\r\n",
    "                min_font_size = 10).generate_from_frequencies(df2)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize = (8, 8), facecolor = None) \r\n",
    "plt.imshow(wordcloud) \r\n",
    "plt.axis(\"off\") \r\n",
    "plt.tight_layout(pad = 0) \r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Exporting the graph\r\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\r\n",
    "plt.axis(\"off\")\r\n",
    "plt.savefig('wordcloud.png', dpi=1000)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71ca7bb33420e5af597533d33abfc83a8213e099b977db95951e7907983ff1c1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "metadata": {
   "interpreter": {
    "hash": "71ca7bb33420e5af597533d33abfc83a8213e099b977db95951e7907983ff1c1"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}